from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# Keywords yang akan di-scrape secara berurutan
KEYWORDS = [
     "Procurement", "Quality Control",
    "Logistics", "Supply Chain", "Maintenance Engineer", "Mechanical Engineer"
]

def keyword_to_url_format(keyword):
    """Convert keyword ke format URL jobstreet"""
    return keyword.lower().replace(" ", "-")

def random_delay(min_sec=0.2, max_sec=0.8):
    delay = random.uniform(min_sec, max_sec)
    print(f"‚è≥ Waiting {delay:.1f} seconds...")
    time.sleep(delay)

def scrape_keyword(keyword):
    """Scrape semua job links untuk keyword tertentu"""
    print(f"\n{'='*60}")
    print(f"üéØ STARTING SCRAPING FOR: {keyword}")
    print(f"{'='*60}")
    
    url_format = keyword_to_url_format(keyword)
    
    options = Options()
    # options.add_argument("--headless")  # Untuk debug, bisa dimatikan dulu
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.101 Safari/537.36")
    service = Service(r"C:\Users\DEMO PAMERAN\OneDrive\Documents\chromedriver-win64\chromedriver.exe")
    driver = webdriver.Chrome(service=service, options=options)

    jobs_data = []
    page_num = 1  

    try:
        while True:  # Loop sampai tidak ada job lagi
            # URL untuk setiap halaman
            if page_num == 1:
                url = f"https://id.jobstreet.com/id/{url_format}-jobs"
            else:
                url = f"https://id.jobstreet.com/id/{url_format}-jobs?page={page_num}"
            
            print(f"\nüìÑ Processing page {page_num}: {url}")
            driver.get(url)
            
            # Delay loading halaman
            print("üîÑ Waiting for page to load...")
            random_delay(0.5, 1.0)

            # Ambil semua job card di halaman saat ini
            job_cards = driver.find_elements(By.CSS_SELECTOR, 'article[data-automation="normalJob"]')
            print(f"‚úÖ Total job cards di halaman {page_num}: {len(job_cards)}")
            
            if len(job_cards) == 0:
                print(f"‚ùå Tidak ada job card di halaman {page_num}. Selesai untuk '{keyword}'.")
                break

            for idx, card in enumerate(job_cards):
                print(f"üîÑ Processing card {idx+1} of {len(job_cards)} (Page {page_num})")
                driver.execute_script("arguments[0].scrollIntoView();", card)
                random_delay(0.2, 0.4)
                # Ambil link langsung dari elemen <a> di dalam card
                try:
                    link_elem = card.find_element(By.TAG_NAME, "a")
                    job_link = link_elem.get_attribute("href")
                    jobs_data.append({
                        "Link": job_link,
                        "Keyword": keyword
                    })
                    print(f"‚úÖ [{len(jobs_data)}] Link berhasil diambil untuk '{keyword}'")
                    # Backup setiap 100 link
                    if len(jobs_data) % 500 == 0:
                        df = pd.DataFrame(jobs_data)
                        filename = f"{url_format}_backup_{len(jobs_data)}.csv"
                        df.to_csv(filename, index=False)
                        print(f"üíæ Backup data ke {filename}")
                    if idx < len(job_cards) - 1:
                        print("‚è≥ Preparing for next job...")
                        random_delay(0.2, 0.5)
                except Exception as e:
                    print(f"‚ùå Gagal mengambil link untuk card {idx+1}: {e}")
                    continue
            print(f"üîÑ Finished page {page_num} untuk '{keyword}'. Preparing for next page...")
            random_delay(0.5, 1.0)
            
            # Increment halaman untuk iterasi berikutnya
            page_num += 1

    finally:
        driver.quit()

    # Tampilkan summary untuk keyword ini
    if jobs_data:
        print(f"\nüìä SUMMARY untuk '{keyword}':")
        print(f"üìÑ Total halaman diproses: {page_num-1}")
        print(f"üîó Total link berhasil diambil: {len(jobs_data)}")
        
        # Simpan ke CSV
        df = pd.DataFrame(jobs_data)
        filename = f"{url_format}.csv"
        df.to_csv(filename, index=False)
        print(f"üíæ Data disimpan ke {filename}")
        
        return filename, len(jobs_data)
    else:
        print(f"‚ùå Tidak ada data untuk '{keyword}'")
        return None, 0

def main():
    """Main function untuk menjalankan scraping semua keywords berurutan"""
    print(f"üöÄ STARTING SEQUENTIAL KEYWORD SCRAPER")
    print(f"üìã Keywords: {' ‚Üí '.join(KEYWORDS)}")
    print(f"‚è∞ Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    total_links = 0
    successful_keywords = []
    failed_keywords = []
    
    for i, keyword in enumerate(KEYWORDS, 1):
        print(f"\nüéØ Progress: {i}/{len(KEYWORDS)} keywords")
        print(f"üîÑ Current: {keyword}")
        
        try:
            filename, link_count = scrape_keyword(keyword)
            if filename:
                successful_keywords.append((keyword, filename, link_count))
                total_links += link_count
                print(f"‚úÖ SUCCESS: '{keyword}' completed with {link_count} links")
            else:
                failed_keywords.append(keyword)
                print(f"‚ùå FAILED: '{keyword}' - no data scraped")
                
        except Exception as e:
            failed_keywords.append(keyword)
            print(f"‚ùå FAILED: '{keyword}' - Error: {e}")
            
        # Delay antar keyword untuk menghindari rate limiting
        if i < len(KEYWORDS):
            print(f"‚è≥ Waiting before next keyword...")
            random_delay(1, 2)
    
    # Final summary
    print(f"\n{'='*80}")
    print(f"üéâ SEQUENTIAL SCRAPING COMPLETED!")
    print(f"{'='*80}")
    print(f"‚è∞ End time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚úÖ Successful keywords: {len(successful_keywords)}")
    print(f"‚ùå Failed keywords: {len(failed_keywords)}")
    print(f"ÔøΩ Total links collected: {total_links}")
    
    if successful_keywords:
        print(f"\nüìä SUCCESSFUL RESULTS:")
        for keyword, filename, count in successful_keywords:
            print(f"  ‚Ä¢ {keyword}: {count} links ‚Üí {filename}")
    
    if failed_keywords:
        print(f"\n‚ùå FAILED KEYWORDS:")
        for keyword in failed_keywords:
            print(f"  ‚Ä¢ {keyword}")
    
    print(f"\nüí° Files generated: Check CSV files for each successful keyword!")

if __name__ == "__main__":
    main()